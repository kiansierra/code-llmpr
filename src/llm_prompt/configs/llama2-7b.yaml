model_org: meta-llama
model_name: Llama-2-7b-hf

model: 
  pretrained_model_name_or_path: ${model_org}/${model_name}
  device_map: auto


trainer:
  output_dir: ../checkpoints/${model_name}
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 8
  save_strategy: steps
  num_train_epochs: 2
  save_steps: 200
  save_total_limit: 3
  overwrite_output_dir: True
  dataloader_num_workers: 8
  dataloader_persistent_workers: True
  optim: 'adamw_8bit'
  group_by_length: True
  length_column_name: 'original_length'

lora:
    r: 8
    lora_alpha: 8
    lora_dropout: 0.1
    target_modules: ["q_proj","k_proj","v_proj","o_proj"]
    bias: "none"
    task_type: "CAUSAL_LM"
