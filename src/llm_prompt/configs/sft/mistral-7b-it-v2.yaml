model_org: mistralai
model_name: Mistral-7B-Instruct-v0.2
output_folder_name: mistral-7b-it-v2

formatter: mistral-message-stack

quantization:
  load_in_4bit: True
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: ${dtype:bf16}

model: 
  pretrained_model_name_or_path: ${model_org}/${model_name}
  torch_dtype: ${dtype:bf16}
  # device_map: auto
  attn_implementation: flash_attention_2


trainer:
  output_dir: ../checkpoints/sft/${output_folder_name}
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 64
  learning_rate: 2e-4
  save_strategy: steps
  num_train_epochs: 2
  logging_steps: 5
  save_steps: 20
  save_total_limit: 2
  evaluation_strategy: steps
  eval_steps: 20
  max_steps: -1
  overwrite_output_dir: True
  dataloader_num_workers: 4
  dataloader_persistent_workers: True
  report_to: wandb
  bf16: True
  optim: 'paged_adamw_8bit'
  group_by_length: True
  length_column_name: 'original_length'

lora:
    r: 32
    lora_alpha: 32
    lora_dropout: 0.1
    target_modules: ["q_proj", "o_proj", "k_proj", "v_proj", "gate_proj", "up_proj", "down_proj"]
    bias: "none"
    task_type: "CAUSAL_LM"
