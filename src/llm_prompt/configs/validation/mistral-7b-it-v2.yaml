model_org: mistralai
model_name: Mistral-7B-Instruct-v0.2
output_folder_name: mistral-7b-it-v2

validation_size: Null

formatter: 
  name: mistral-message-stack
  system: mock
  num_examples: 2

quantization:
  load_in_4bit: True
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: ${dtype:bf16}

model: 
  pretrained_model_name_or_path: ${model_org}/${model_name}
  torch_dtype: ${dtype:bf16}
  attn_implementation: flash_attention_2

generation:
  max_new_tokens: 30
